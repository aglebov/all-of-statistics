{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa944f29-5b94-4521-bc71-7f04deb3e40f",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "(a) By Theorem 3.17 or Exercise 8 in Chapter 3.\n",
    "\n",
    "(b) We have\n",
    "$$S_n^2 \n",
    "= \\frac{1}{n - 1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2\n",
    "= \\frac{1}{n - 1}\\sum_{i=1}^n (X_i^2 - 2 X_i \\bar{X}_n + \\bar{X}_n^2)\n",
    "= \\frac{1}{n - 1}\\left( \\sum_{i=1}^n X_i^2 - 2 \\bar{X}_n \\sum_{i=1}^n X_i + n\\bar{X}_n^2 \\right)\n",
    "= \\frac{1}{n - 1}\\left( \\sum_{i=1}^n X_i^2 - n \\bar{X}_n^2\\right)\n",
    "= \\frac{n}{n - 1} \\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\frac{n}{n-1} \\bar{X}_n^2.\n",
    "$$\n",
    "\n",
    "By Theorem 5.6,\n",
    "$$\\frac{1}{n} \\sum_{i=1}^n X_i^2 \\xrightarrow{P} \\mathbb{E}[X_i^2] = \\sigma^2 + \\mu^2.$$\n",
    "Trivially,\n",
    "$$\\frac{n}{n-1} \\xrightarrow{P} 1,$$\n",
    "so from Theorem 5.5(d), we get\n",
    "$$\\frac{n}{n - 1} \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\xrightarrow{P} \\sigma^2 + \\mu^2.$$\n",
    "\n",
    "Also by Theorem 5.6,\n",
    "$$\\bar{X}_n \\xrightarrow{P} \\mu,$$\n",
    "so by Theorem 5.5(d),\n",
    "$$\\bar{X}_n^2 \\xrightarrow{P} \\mu^2$$\n",
    "and\n",
    "$$\\frac{n}{n-1} \\bar{X}_n^2 \\xrightarrow{P} \\mu^2.$$\n",
    "\n",
    "Finally, by Theorem 5.5(a), we obtain\n",
    "$$\\frac{n}{n - 1} \\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\frac{n}{n-1} \\bar{X}_n^2 \\xrightarrow{P} \\sigma^2 + \\mu^2 - \\mu^2 = \\sigma^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071526a-fb91-4578-ad79-63402270d8c1",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Note that\n",
    "$$\\begin{split}\n",
    "\\mathbb{E}[(X_n - b)^2] \n",
    "&= \\mathbb{E}\\left[(X_n - \\mathbb{E}(X_n) + \\mathbb{E}[X_n] - b)^2\\right] \\\\\n",
    "&= \\mathbb{E}\\left[(X_n - \\mathbb{E}(X_n))^2\\right] + 2 \\mathbb{E}[(X_n - \\mathbb{E}(X_n)](\\mathbb{E}[X_n] - b) + (\\mathbb{E}[X_n] - b)^2 \\\\\n",
    "&= \\mathbb{E}\\left[(X_n - \\mathbb{E}(X_n))^2\\right] + (\\mathbb{E}[X_n] - b)^2, \\\\\n",
    "\\end{split}$$\n",
    "from which\n",
    "$$\\begin{split}\n",
    "\\mathbb{E}\\left[(X_n - \\mathbb{E}(X_n))^2\\right] &\\leq \\mathbb{E}[(X_n - b)^2] \\\\\n",
    "(\\mathbb{E}[X_n] - b)^2 &\\leq \\mathbb{E}[(X_n - b)^2].\n",
    "\\end{split}$$\n",
    "\n",
    "Therefore, if $X_n \\xrightarrow{qm} b$, i.e. $\\mathbb{E}[(X_n - b)^2] \\to 0$ as $n \\to \\infty$, we have $\\mathbb{E}\\left[(X_n - \\mathbb{E}(X_n))^2\\right] \\to 0$ and $(\\mathbb{E}[X_n] - b)^2 \\to 0$, which means $\\mathbb{V}[X_n] \\to 0$ and $\\mathbb{E}[X_n] \\to b$, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed7123-7b0f-4edb-8af2-90dfc936bf4b",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Since $X_1, \\dots, X_n$ are i.i.d., we have\n",
    "$$\\text{Cov}(X_i, X_j) = \\begin{cases}\n",
    "\\sigma^2, &\\text{ if } i = j, \\\\\n",
    "0, &\\text{ if } i \\neq j, \\\\\n",
    "\\end{cases}$$\n",
    "so\n",
    "$$\\mathbb{E}[X_i X_j] \n",
    "= \\text{Cov}(X_i, X_j) + \\mathbb{E}[X_i] \\mathbb{E}[X_j]\n",
    "= \\text{Cov}(X_i, X_j) + \\mu^2\n",
    "= \\begin{cases}\n",
    "\\sigma^2 + \\mu^2, &\\text{ if } i = j, \\\\\n",
    "\\mu^2, &\\text{ if } i \\neq j. \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "By Theorem 3.17, $\\mathbb{E}[\\bar{X}_n] = \\mu$.\n",
    "\n",
    "Now,\n",
    "$$\\mathbb{E}[\\bar{X}_n^2]\n",
    "= \\mathbb{E}\\left[\\left(\\frac{1}{n} \\sum_{i=1}^n X_i \\right)^2 \\right] \n",
    "= \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\mathbb{E} [X_i X_j]\n",
    "= \\frac{1}{n^2} (n (\\sigma^2 + \\mu^2) + n(n-1) \\mu^2)\n",
    "= \\frac{1}{n^2} (n \\sigma^2 + n^2 \\mu^2)\n",
    "= \\frac{\\sigma^2}{n} + \\mu^2,$$\n",
    "so\n",
    "$$\\mathbb{V}[\\bar{X}_n] = \\mathbb{E}[\\bar{X}_n^2] - \\mathbb{E}[\\bar{X}_n]^2 = \\frac{\\sigma^2}{n}.$$\n",
    "\n",
    "We see that \n",
    "$$\\lim_{n\\to\\infty} \\mathbb{V}[\\bar{X}_n] = 0$$\n",
    "and\n",
    "$$\\lim_{n\\to\\infty} \\mathbb{E}[\\bar{X}_n] = \\mu,$$\n",
    "so using the results of Exercise 2, we conclude that $\\bar{X}_n \\xrightarrow{qm} \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627d4b7-8af4-4fb1-8faf-a67128e2bda0",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "For any $\\varepsilon$, there exists $n_\\varepsilon = \\lceil \\frac{1}{\\varepsilon} \\rceil$ such that for all $n \\geq n_\\varepsilon$ we have $\\varepsilon < \\frac{1}{n}$, so\n",
    "$$\\mathbb{P}[X_n > \\varepsilon] = \\frac{1}{n^2} \\to 0$$\n",
    "for $n \\to \\infty$, therefore we have $X_n \\xrightarrow{P} 0$.\n",
    "\n",
    "Since convergence in quadratic mean implies convergence in probability, and $X_n$ converges in probability to 0, it can only converge in quadratic mean to 0.\n",
    "\n",
    "Now,\n",
    "$$\\mathbb{E}[X_n] = \\frac{1}{n}\\left(1 - \\frac{1}{n^2}\\right) + n \\frac{1}{n^2} = \\frac{2}{n} - \\frac{1}{n^3} = \\frac{1}{n} \\left( 2 + \\frac{1}{n^2}\\right)$$\n",
    "and\n",
    "$$\\mathbb{E}[X_n^2] = \\frac{1}{n^2}\\left(1 - \\frac{1}{n^2}\\right) + n^2 \\frac{1}{n^2} = \\frac{1}{n^2}\\left(1 - \\frac{1}{n^2}\\right) + 1,$$\n",
    "so\n",
    "$$\\mathbb{V}[X_n] = \\mathbb{E}[X_n^2] - \\mathbb{E}[X_n]^2 = \\frac{1}{n^2}\\left(1 - \\frac{1}{n^2}\\right) + 1 - \\frac{1}{n^2} \\left( 2 + \\frac{1}{n^2}\\right)^2.$$\n",
    "Clearly, $\\mathbb{V}[X_n] \\to 1$ as $n \\to \\infty$, so by the results of Exercise 2, we $X_n$ does not converge in quadratic mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8db733-8e55-411d-b063-1e2ca5b88f50",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Note that if $X \\sim \\text{Bernoulli}(p)$, then also $X^2 \\sim \\text{Bernoulli}(p)$, so it is sufficient to prove the statements for $X_i$.\n",
    "\n",
    "We have\n",
    "$$\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\mathbb{E}[X_i] = p$$\n",
    "and\n",
    "$$\\mathbb{V}\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{\\mathbb{V}[X_i]}{n} = \\frac{p(1-p)}{n} \\to 0$$\n",
    "as $n \\to \\infty$. Using the results of Exercise 2, we get $\\frac{1}{n} \\sum_{i=1}^n X_i \\xrightarrow{qm} p$, and by Theorem 5.4(a), $\\frac{1}{n} \\sum_{i=1}^n X_i \\xrightarrow{P} p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c658b5-9c92-49af-8d38-b99a7a8b3656",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "From the Central Limit Theorem (Theorem 5.8), we have that the average has approximately normal distribution with mean 68 and standard deviation $2.6 / \\sqrt{n}$. Since the normal distribution is symmetric about its mean, the probability that the average will be above the mean is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b394e-d01f-4368-8882-3884d4c1e321",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "(a) If $X_n \\sim \\text{Poisson}(\\lambda_n)$, then $\\mathbb{E}[X_n] = \\frac{1}{n} \\to 0$ and $\\mathbb{V}[X_n] = \\frac{1}{n} \\to 0$ as $n \\to \\infty$. Then by the results of Exercise 2, \n",
    "$X_n \\xrightarrow{qm} 0$ and by Theorem 5.4(a), $X_n \\xrightarrow{P} 0$.\n",
    "\n",
    "(b) We have $\\mathbb{E}[nX_n] = 1$ and $\\mathbb{V}[nX_n] = n$, so by the results of Exercise 2, $Y_n = n X_n$ does not converge in quadratic mean. We show that it nonetheless converges in probability. For small $\\varepsilon$, we have\n",
    "$$\\mathbb{P}[n X_n > \\varepsilon] = 1 - \\mathbb{P}[X_n = 0] = 1 - e^{-1/n} \\to 0$$\n",
    "as $n \\to \\infty$, so by definition $n X_n \\xrightarrow{P} 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64547c48-b500-450b-aea4-8cd81c2728a3",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "Since $X_i \\sim \\text{Poisson}(\\lambda)$, we have $\\mathbb{E}[X_i] = \\lambda$ and $\\mathbb{V}[X_i] = \\lambda$. Then using the Central Limit Theorem (Theorem 5.8),\n",
    "$$\\frac{\\sqrt{n}}{\\sqrt{\\lambda}} \\left(\\frac{1}{n} Y - \\lambda\\right) \\sim N(0, 1).$$\n",
    "Then\n",
    "$$\\mathbb{P}[Y < a] \n",
    "= \\mathbb{P}\\left[\\frac{\\sqrt{n}}{\\sqrt{\\lambda}} \\left(\\frac{1}{n} Y - \\lambda\\right) <  \\frac{\\sqrt{n}}{\\sqrt{\\lambda}} \\left(\\frac{a}{n} - \\lambda\\right)\\right]\n",
    "= \\mathbb{P}\\left[Z < \\frac{\\sqrt{n}}{\\sqrt{\\lambda}} \\left(\\frac{a}{n} - \\lambda\\right)\\right]\n",
    "= \\Phi\\left( \\frac{\\sqrt{n}}{\\sqrt{\\lambda}} \\left(\\frac{a}{n} - \\lambda\\right) \\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "356bdd2a-5f9c-4b37-9f8d-c55167ac53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3ec4df-da93-4fc6-85f1-b164b6a3c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "a = 90\n",
    "lam = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92d7265-38d7-4cb5-a5ef-a2bdca24a7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.15865525393145707)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.norm.cdf((a / n - lam) * np.sqrt(n / lam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9873e8-c371-4fc5-9101-6b4604a46579",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "For small $\\varepsilon$, we have \n",
    "$$\\mathbb{P}[|X_n - X| > \\varepsilon] = \\frac{1}{n} \\to 0$$\n",
    "as $n \\to \\infty$, hence $X_n$ converges to $X$ in probability, and by Theorem 5.4(b), also $X_n$ converges to $X$ in distribution.\n",
    "\n",
    "On the other hand,\n",
    "$$\\mathbb{E}[(X - X_n)^2] = \\frac{1}{n}\\left(\\frac{1}{2} (e^n + 1)^2 + \\frac{1}{2} (e^n - 1)^2\\right) = \\frac{e^n + 1}{n} \\to \\infty$$\n",
    "as $n \\to \\infty$, hence $X_n$ does not converge to $X$ in quadratic mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19aab29-c074-40ef-8e32-93f9fdcc5351",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "\n",
    "Using Markov inequality, we get\n",
    "$$\\mathbb{P}[|Z| > t] = \\mathbb{P}[|Z|^k > t^k] \\leq \\frac{\\mathbb{E}|Z|^k}{t^k}.$$\n",
    "Note that the assumption $Z \\sim N(0, 1)$ is redundant here. The comparison for $Z \\sim N(0, 1)$ was done in Exercise 6 in Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8338a-624f-4056-a932-95e8b9bbf742",
   "metadata": {},
   "source": [
    "## Exercise 11\n",
    "\n",
    "$X$ is a point mass at $0$, so we have $\\mathbb{P}[|X_n| > \\varepsilon] = \\mathbb{P}[\\sqrt{n}|X_n| > \\varepsilon\\sqrt{n}] = 2 \\Phi(-\\varepsilon\\sqrt{n}) \\to 0$ as $n \\to \\infty$, so $X_n \\xrightarrow{P} X$. By Theorem 5.4(b), $X_n \\rightsquigarrow X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a0ffb-658b-4376-a879-75150ae6c323",
   "metadata": {},
   "source": [
    "## Exercise 12\n",
    "\n",
    "Since $X, X_1, X_2, \\dots$ are discrete, they can only have discontinuities at interer points and also\n",
    "$$\\begin{split}\n",
    "\\mathbb{P}[X = k] &= F(k + 0.5) - F(k - 0.5), \\\\\n",
    "\\mathbb{P}[X_n = k] &= F_n(k + 0.5) - F_n(k - 0.5),\n",
    "\\end{split}$$\n",
    "where $F$ and $F_n$ are CDFs of $X$ and $X_n$, respectively. Since $X, X_1, X_2, \\dots$ are positive, we have $F_n(x) = F(x) = 0$ for $x < 1$ and $\\mathbb{P}[X_n = k] = \\mathbb{P}[X=k] =0$ for $k \\leq 0$.\n",
    "\n",
    "We prove the required statement by induction. Suppose for all integer continuity points of $F$ we have\n",
    "$$\\lim_{n \\to \\infty} F_n(x) = F(x),$$\n",
    "in particular for all $k$\n",
    "$$\\lim_{n \\to \\infty} F_n(k + 0.5) = F(k + 0.5).$$\n",
    "Then \n",
    "$$\\mathbb{P}[X_n = k] = F_n(k + 0.5) - F_n(k - 0.5) \\to F(k + 0.5) - F(k - 0.5) = \\mathbb{P}[X = k],$$\n",
    "as required.\n",
    "\n",
    "Now suppose\n",
    "$$\\lim_{n\\to\\infty} \\mathbb{P}[X_n = k] = \\mathbb{P}[X=k]$$\n",
    "for $k \\geq 1$. For any $\\varepsilon \\in (0, 1)$ we have\n",
    "$$F_n(1 + \\varepsilon) = \\mathbb{P}[X_n = 1] \\to \\mathbb{P}[X = 1] = F(1 + \\varepsilon).$$\n",
    "Then\n",
    "$$F_n(2 + \\varepsilon) = F_n(1 + \\varepsilon) + \\mathbb{P}[X_n = 2] \\to F(1 + \\varepsilon) + \\mathbb{P}[X = 2] = F(2 + \\varepsilon),$$\n",
    "and continue in this fashion to show by induction that\n",
    "$$F_n(k + \\varepsilon) \\to F(k + \\varepsilon)$$\n",
    "for all $k \\geq 1$. By definition then $X_n \\rightsquigarrow X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d303a6-9cdf-44e8-ab64-cc0a24ff25ca",
   "metadata": {},
   "source": [
    "## Exercise 13\n",
    "\n",
    "Since $Z_1, \\dots, Z_n$ are independent,\n",
    "$$F_n(x) = \\mathbb{P}[X_n \\leq x] \n",
    "= \\mathbb{P}[n \\min\\{Z_1, \\dots, Z_n\\} \\leq x]\n",
    "= 1 - \\mathbb{P}\\left[Z_1 > \\frac{x}{n}, \\dots, Z_n > \\frac{x}{n}\\right] \n",
    "= 1 - \\mathbb{P}\\left[Z_1 > \\frac{x}{n}\\right]^n\n",
    "= 1 - \\left(1 - F\\left(\\frac{x}{n}\\right)\\right)^n,$$\n",
    "where $F_n$ is the CDF of $X_n$ and $F$ is the CDF of $Z_1$.\n",
    "\n",
    "Since $\\lim_{x \\downarrow 0} f(x) = \\lambda$, we have $\\forall \\varepsilon > 0 \\;\\; \\exists \\delta_\\varepsilon \\;\\; \\forall x \\in (0, \\delta_\\varepsilon) \\;\\; |f(x) - \\lambda| < \\varepsilon$. Let's pick $\\varepsilon$ and find the corresponding $\\delta_\\varepsilon$.\n",
    "\n",
    "Then for all $n \\geq n_\\delta = \\lceil x / \\delta_\\varepsilon \\rceil$ and $t \\leq \\frac{x}{n}$,  $t \\in (0, \\delta_\\varepsilon)$, and thus $\\lambda - \\varepsilon < f(t) < \\lambda + \\varepsilon$. We can then bound the integral\n",
    "$$F\\left(\\frac{x}{n}\\right) = \\int_0^{\\frac{x}{n}} f(t) dt$$\n",
    "as follows:\n",
    "$$(\\lambda - \\varepsilon) \\frac{x}{n} \\leq \\int_0^{\\frac{x}{n}} f(t) dt \\leq (\\lambda + \\varepsilon) \\frac{x}{n},$$\n",
    "which gives us\n",
    "$$1 - \\left(1 - (\\lambda - \\varepsilon)\\frac{x}{n}\\right)^n \\leq 1 - \\left(1 - F\\left(\\frac{x}{n}\\right)\\right)^n \\leq 1 - \\left(1 - (\\lambda + \\varepsilon)\\frac{x}{n}\\right)^n.$$\n",
    "Taking the limit for $n \\to \\infty$, we have\n",
    "$$1 - e^{-(\\lambda - \\varepsilon)x} \\leq \\lim_{n \\to \\infty} \\left( 1 - \\left(1 - F\\left(\\frac{x}{n}\\right)\\right)^n \\right) \\leq 1 - e^{-(\\lambda + \\varepsilon)x},$$\n",
    "and since this holds for all $\\varepsilon$,\n",
    "$$1 - e^{-\\lambda x} \\leq \\lim_{n \\to \\infty} \\left( 1 - \\left(1 - F\\left(\\frac{x}{n}\\right)\\right)^n \\right) \\leq 1 - e^{-\\lambda x},$$\n",
    "therefore\n",
    "$$\\lim_{n \\to\\infty} F_n(x) = 1 - e^{-\\lambda x},$$\n",
    "which is the CDF of the exponential distribution with mean $1 / \\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5d8d0-4509-47b1-994b-a7ad209d192d",
   "metadata": {},
   "source": [
    "## Exercise 14\n",
    "\n",
    "First, $\\mathbb{E}[X_i] = \\frac{1}{1}$ and $\\mathbb{V}[X_i] = \\frac{1}{12}$. By the Central Limit Theorem (Theorem 5.8), we have\n",
    "$$\\bar{X}_n \\approx N\\left(\\frac{1}{2}, \\frac{1}{12n}\\right).$$\n",
    "Using the Delta Method (Theorem 5.13) with $g(x) = x^2$ then gives us\n",
    "$$\\bar{X}_n^2 \\approx N\\left(\\frac{1}{4}, \\frac{1}{12n} \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed9888-017f-4152-9be7-8fdcd4ab0c79",
   "metadata": {},
   "source": [
    "## Exercise 15\n",
    "\n",
    "We have \n",
    "$$g(s_1, s_2) = \\frac{s_1}{s_2},$$\n",
    "so\n",
    "$$\\nabla g = \\begin{pmatrix}\n",
    "\\frac{1}{s_2} \\\\\n",
    "-\\frac{s_1}{s_2^2}\n",
    "\\end{pmatrix}$$\n",
    "and evaluated at $(\\mu_1, \\mu_2)$:\n",
    "$$\\nabla_\\mu = \\nabla g |_{(\\mu_1, \\mu_2)} = \\begin{pmatrix}\n",
    "\\frac{1}{\\mu_2} \\\\\n",
    "-\\frac{\\mu_1}{\\mu_2^2}\n",
    "\\end{pmatrix}.$$\n",
    "Then by Theorem 5.15,\n",
    "$$Y_n \\approx N\\left( \\frac{\\mu_1}{\\mu_2}, \\frac{\\nabla_\\mu^T \\Sigma \\nabla_\\mu}{n}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1b250-a7e4-48f2-a5e3-3b21161906db",
   "metadata": {},
   "source": [
    "## Exercise 16\n",
    "\n",
    "Let $X \\sim N(0, 1)$, $X_n = -X$, $Y_n = X$ and $Y=X$. Then $X_n \\rightsquigarrow X$ and $Y_n \\rightsquigarrow Y = X$, but $X_n + Y_n = 0$ does not converge in distribution to $X + Y = 2X$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
